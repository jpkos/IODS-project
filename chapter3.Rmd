# Chapter 3: Logistic regression practice

Let's start by loading the necessary libraries
```{r, results='hide', message=FALSE}
library(dplyr)
library(GGally)
library(ggplot2)
library(tidyr)
library(caret)
```
## Dataset description
Load the data we processed in the data wrangling section (actually, I'm using the ready made dataset)
```{r}
data <- read.csv("data/student/alc.csv")
```
Print a summary of the data structure
```{r}
str(data)
```
The data describes each subjects school performance together with different study and personal information such as absences (from school), failures (at school), age, family size, education level, etc. G1, G2 and G3 are the grades in three different periods for the given course subject (portuguese or math). Alcohol level is a binary variable (high/low usage).

# Analysis
We will analyse students' alcohol consumption and its effects on 4 different variables. The dependent variable is high alcohol use (high_use), with the following independent variables and the hypotheses:

1) Absences: Students that have more absences are more likely to have high alcohol use
2) Failures: Students with more failues are more likely to have high alcohol use
3) Sex: Men are more likely to have higher alcohol use
4) Romantic: People who are in a relationship are less likely to have high alcohol use

First, let's plot the distributions of these variables:
```{r}
cols <- c("sex", "failures", "absences", "romantic", "high_use")
gather(data[cols]) %>% ggplot(aes(value)) + facet_wrap("key", scales="free") + geom_bar()
```

Fit a model with these variables:
```{r}
model <- glm(high_use ~ absences + failures + sex + romantic, data=data, family="binomial")
summary(model)
```
According to the results:

- Absences correlated with higher probability of high alcohol use, the log odds increase by 0.094, and the increase is statistically significant (z = 4.049, p<0.001).
- Failures correlated with higher probability of igh alcohol use, log odds increase by 0.605, and the increase is statistically significant (z = 2.910, p=0.004).
- Men had had higher probability of high alcohol use, log odds increase by 0.98, and the increase is statistically significant4 (z = 3.969, p<0.001).
- Being in a relationship decreased the chance of higher alcohol use, log odds decrease by -0.246, but the change was not significant, (-0.924, p=0.356).

Hypotheses 1 - 3 were supported by the data. For hypothesis 4, the effect was negative as predicted, but the effect is not statistically significant.


The confidence intervals of these variables are:
```{r}
confint(model)
```
We can also interpret the coefficients as odds ratios by running them through the exponent function
```{r}
coef(model) %>% exp
```
Now the coefficients indicate e.g. being Male increases the odds of high alcohol use by 2.675, and so on.

Let's compare predicted vs real results with a 2x2 table
```{r}
predictions <- as.factor(predict(model, newdata = data, type = "response")>0.5)
true = as.factor(data$high_use)
confusionMatrix(data=predictions, reference=true)
```
Total proportion of inaccurate results: 77 + 12 = 89. Total number of values was 370, so 89/370 = 0.2405 or about 24% of the data was predicted with the wrong label.

To see if this is a good result, we consider what result could be achieved with random chance. If we just flipped coins, we would get 50% correct. The logistic regression model is clearly better than that. *However*, we can also see that the proportion of non-high users is much larger than the proportion of high alcohol users. There are 259 non-high users versus 111 high users. Thus, ff we had a model that predicted everyone as non-high user, we would get 70% correct. Therefore, we can see that the model is actually not that much better than a model "no one is high user".



